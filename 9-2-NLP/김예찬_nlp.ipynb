{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQx1Dj7XbS-i"
      },
      "source": [
        "# **9. NLP 세션 과제**\n",
        "\n",
        "- 이번 과제는 BPE 토크나이저를 직접 구현해보고, 훈련시켜보는 것입니다.\n",
        "\n",
        "- **각 셀의 실행 결과물을 같이** 저장해서, 드라이브에 업로드 된 과제 명세에 적혀있는 제출 방식을 참고하여 본 노트북 파일을 제출해주세요. </br></br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### **주의사항!**\n",
        "\n",
        "- BPE를 구현하는 코드는 조금만 검색해도 쉽게 찾을 수 있습니다. (유명하니까요...) 하지만 해당 코드를 먼저 찾아보는 것은 추천하지 않습니다.\n",
        "\n",
        "- 본 과제의 목적은 구현 능력을 기르는 데 있습니다. 이론을 실제 코드로 구현하는 능력이 있고, 없고는 굉장히 큰 차이입니다!!\n",
        "\n",
        "- 따라서, 본 과제를 수행할 때, **BPE의 이론을 먼저\n",
        "충분히 복습**하고, **다른 샘플 코드를 보지 않고** 각각의 기능을 어떻게 구현할지 충분히 고민해보시기 바랍니다!\n",
        "\n",
        "- 인터넷에 샘플 코드가 많다는 건 여러분들도 충분히 하실 수 있다는 의미니까요!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUAzagMqbZad"
      },
      "source": [
        "## 1. BPE Tokenizer 구현하기 (70점, 각 세부 파트별 부분점수 있음)\n",
        "\n",
        "  **BPE Tokenizer**를 구현하기 위해서, 크게 다음 **세가지** 부분으로 쪼갤 수 있습니다.\n",
        "\n",
        "```\n",
        "  1) 전체 corpus를 공백으로 분리하고, {단어:빈도수} 형태의 vocabulary 사전을 구성하는 부분\n",
        "\n",
        "  2) 구성된 vocabulary 사전을 순회하면서, 사전 내 character 토큰 및 각각의 등장 횟수를 반환해 주는 부분\n",
        "\n",
        "  3) text에서 가장 자주 등장한 pair을 연결해주는 부분\n",
        "```\n",
        "\n",
        "본 과제에서 하실 일은 각각의 함수 코드를 직접 작성해보고, 각각의 함수를 모두 합쳐 최종적으로 BPE Tokenizer를 구현하는 것입니다!\n",
        "\n",
        "**Hint**: Collections 패키지의 defaultdict 클래스의 활용법을 찾아보시면 좀 더 손쉽게 구현할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "P8viuw3utO0f"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzvipA3QqYth"
      },
      "source": [
        "여러분이 작성한 코드가 맞는지 스스로 감을 잡으실 수 있도록, 각 함수 별로 샘플 corpus를 함수에 넣었을 때 나와야 하는 결과를 함께 제시하도록 하겠습니다. 샘플 corpus는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fAVFREwObZAR"
      },
      "outputs": [],
      "source": [
        "sample_corpus =  [ 'YBAGTA YBECTA YBIGTA YBOGTA YBUGTA',\n",
        "         'I love YBIGTA',\n",
        "          'I like YBIGTA',\n",
        "          'what is YBAGTA']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJaDz00rq8y5"
      },
      "source": [
        "### 1) 사전만들기 함수 (20점)\n",
        "\n",
        " 첫번째 함수는 corpus가 입력으로 주어졌을 때, 이에 기반하여 Vocabulary 사전을 구축하는 함수입니다. 이때, input/output 형태는 다음과 같습니다.\n",
        "\n",
        "\n",
        "  - `input` : 여러 개의 문자열로 이루어진 리스트 (sample corpus와 형태 동일)\n",
        "  - `output` : 여러 개의 단어:빈도수 쌍으로 이루어진 딕셔너리 (예시: `{단어1:빈도수1, 단어2:빈도수2, ... }` )\n",
        "\n",
        "\n",
        "\n",
        "**TODO**:\n",
        "  input 리스트 안의 문자열들에 대해서, 공백을 기준으로 분리한 각각의 단어와 그 빈도수를 딕셔너리 형태로 리턴하는 함수를 정의하세요.\n",
        "\n",
        " 이때, 추후 두번째, 세번째 함수 기능 구현의 편의성을 위해, output 딕셔너리 내 단어 형식을 다음과 같이 맞춰주세요,\n",
        "- 예시: `'Y B I G T A </w>'` -> 글자 사이 공백 추가, 마지막에 postfix 토큰  `</w>` 추가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UxI-cMFMst4H"
      },
      "outputs": [],
      "source": [
        "def 사전만들기(corpus):\n",
        "\n",
        "  ##### TODO #####\n",
        "  # 함수 안을 채워주세요 #\n",
        "  # 이미 작성된 코드는 지우시면 안됩니다! #\n",
        "  vocab = defaultdict(int)\n",
        "  for text in corpus:\n",
        "      words = text.split()  # 공백을 기준으로 분리\n",
        "      for word in words:\n",
        "          # 글자 사이 공백 추가 및 마지막에 </w> 추가\n",
        "          formatted_word = ' '.join(list(word)) + ' </w>'\n",
        "          if formatted_word in vocab:\n",
        "              vocab[formatted_word] += 1\n",
        "          else:\n",
        "              vocab[formatted_word] = 1\n",
        "    \n",
        "  vocab= dict(vocab)\n",
        "                \n",
        "  return vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvpUlmehueuZ"
      },
      "source": [
        "위에서 정의한 `사전만들기` 함수를 sample_corpus에 적용시 다음과 같은 결과가 리턴되어야 합니다.\n",
        "\n",
        "\n",
        "- 입력:\n",
        "```\n",
        "vocab = 사전만들기(sample_corpus)\n",
        "vocab\n",
        "```\n",
        "\n",
        "- 결과:\n",
        "```\n",
        "{'Y B A G T A </w>': 2,\n",
        " 'Y B E C T A </w>': 1,\n",
        " 'Y B I G T A </w>': 3,\n",
        " 'Y B O G T A </w>': 1,\n",
        " 'Y B U G T A </w>': 1,\n",
        " 'I </w>': 2,\n",
        " 'l o v e </w>': 1,\n",
        " 'l i k e </w>': 1,\n",
        " 'w h a t </w>': 1,\n",
        " 'i s </w>': 1}\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFF9e-zAtCpk",
        "outputId": "862d5287-d978-4062-945a-e7e519965112"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Y B A G T A </w>': 2,\n",
              " 'Y B E C T A </w>': 1,\n",
              " 'Y B I G T A </w>': 3,\n",
              " 'Y B O G T A </w>': 1,\n",
              " 'Y B U G T A </w>': 1,\n",
              " 'I </w>': 2,\n",
              " 'l o v e </w>': 1,\n",
              " 'l i k e </w>': 1,\n",
              " 'w h a t </w>': 1,\n",
              " 'i s </w>': 1}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 다음 코드를 실행한 결과값을 위의 결과와 비교해서 함수를 잘 작성했는지 확인해보세요!\n",
        "vocab = 사전만들기(sample_corpus)\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnTUlA3ywksi"
      },
      "source": [
        "### 2) 토큰등장횟수 함수 (20점)\n",
        "\n",
        "다음으로는, `사전만들기`함수로 얻은 vocab 딕셔너리를 입력을 받아서, 사전에 포함된 글자 각각의 등장 횟수를 반환해주는 함수를 구현해 봅시다.\n",
        "\n",
        "인풋, 아웃풋 형태는 다음과 같습니다.\n",
        "\n",
        "- `input` : `사전만들기`함수의 output\n",
        "- `output`: {글자:빈도수} 구조의 tokens 딕셔너리,\n",
        "    예) {'Y':4, 'B':8, .... }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sB1gtxnBtGyA"
      },
      "outputs": [],
      "source": [
        "def 토큰등장횟수(vocab):\n",
        "\n",
        "  ##### TODO #####\n",
        "  # 함수 안을 채워주세요 #\n",
        "  # 이미 작성된 코드는 지우시면 안됩니다! #\n",
        "  result = defaultdict(int)\n",
        "  for word, freq in vocab.items():\n",
        "        chars = word.split()  # 단어를 글자 단위로 분리\n",
        "        for char in chars:\n",
        "            if char in result:\n",
        "                result[char] += freq\n",
        "            else:\n",
        "                result[char] = freq\n",
        "\n",
        "  result = dict(result)\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gYNuT91mAwb"
      },
      "source": [
        "첫번째로 정의한 `사전만들기`함수에서 반환된 vocab을 `토큰등장횟수`에 넣었을때 반환되는 결과는 다음과 같아야 합니다.\n",
        "\n",
        "```\n",
        "{'Y': 8,\n",
        " 'B': 8,\n",
        " 'A': 10,\n",
        " 'G': 7,\n",
        " 'T': 8,\n",
        " '</w>': 14,\n",
        " 'E': 1,\n",
        " 'C': 1,\n",
        " 'I': 5,\n",
        " 'O': 1,\n",
        " 'U': 1,\n",
        " 'l': 2,\n",
        " 'o': 1,\n",
        " 'v': 1,\n",
        " 'e': 2,\n",
        " 'i': 2,\n",
        " 'k': 1,\n",
        " 'w': 1,\n",
        " 'h': 1,\n",
        " 'a': 1,\n",
        " 't': 1,\n",
        " 's': 1}\n",
        " ```\n",
        "\n",
        " 위와 같은 결과가 나오는지, 다음 셀을 실행하여 확인해볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qalAYLn4z9to",
        "outputId": "d5dbcac2-1d6c-4cee-b59c-0a780c22d8c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Y': 8,\n",
              " 'B': 8,\n",
              " 'A': 10,\n",
              " 'G': 7,\n",
              " 'T': 8,\n",
              " '</w>': 14,\n",
              " 'E': 1,\n",
              " 'C': 1,\n",
              " 'I': 5,\n",
              " 'O': 1,\n",
              " 'U': 1,\n",
              " 'l': 2,\n",
              " 'o': 1,\n",
              " 'v': 1,\n",
              " 'e': 2,\n",
              " 'i': 2,\n",
              " 'k': 1,\n",
              " 'w': 1,\n",
              " 'h': 1,\n",
              " 'a': 1,\n",
              " 't': 1,\n",
              " 's': 1}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 다음 코드를 실행한 결과값을 위의 결과와 비교해서 함수를 잘 작성했는지 확인해보세요!\n",
        "result = 토큰등장횟수(vocab)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O9lm0cK0wn2"
      },
      "source": [
        "### 3) 페어합치기 함수 (30점)\n",
        "\n",
        "마지막 함수입니다. 마지막으로, corpus에서 가장 자주 등장한 페어를 엮어 새롭게 vocab에 추가하는 기능을 구현해봅시다.\n",
        "\n",
        "이를 구현하기 위해서, 다시 다음 두 단계로 쪼갤 수 있습니다.\n",
        "\n",
        "1) `페어등장횟수` (15점): 각 페어가 등장한 횟수를 세어 딕셔너리 형태로 반환\n",
        "- `input` : vocab\n",
        "- `output`: {(글자1,글자2):등장횟수, (글자2,글자3):등장횟수,...} 형식의 딕셔너리\n",
        " -> 아래 샘플 결과 형태를 확인하세요.\n",
        "\n",
        "\n",
        "2) `페어합치기` (15점):\n",
        "-   `input`: vocab, 가장 자주 등장한 Tuple 형태의 pair\n",
        "- `output`: 새롭게 업데이트한 tokens 사전\n",
        "\n",
        "\n",
        "먼저, `페어등장횟수`함수부터 구현해 봅시다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1XCrMW9U0BdX"
      },
      "outputs": [],
      "source": [
        "def 페어등장횟수(vocab):\n",
        "\n",
        "  ##### TODO #####\n",
        "  # 함수 안을 채워주세요 #\n",
        "  # 이미 작성된 코드는 지우시면 안됩니다! #\n",
        "  pairs = defaultdict(int)\n",
        "  for word, freq in vocab.items():\n",
        "        # 단어를 공백으로 분리하여 토큰 리스트로 변환\n",
        "        tokens = word.split()\n",
        "        \n",
        "        # 토큰 리스트에서 각 쌍의 빈도수 계산\n",
        "        for i in range(len(tokens) - 1):\n",
        "            pair = (tokens[i], tokens[i + 1])\n",
        "            pairs[pair] += freq\n",
        "  \n",
        "  pairs = dict(pairs)\n",
        "\n",
        "  return pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVtbBVjarbJv"
      },
      "source": [
        "위 함수에 우리의 예시를 넣어보면 다음과 같은 결과가 나와야 합니다.\n",
        "\n",
        "```\n",
        "{('Y', 'B'): 8,\n",
        " ('B', 'A'): 2,\n",
        " ('A', 'G'): 2,\n",
        " ('G', 'T'): 7,\n",
        " ('T', 'A'): 8,\n",
        " ('A', '</w>'): 8,\n",
        " ('B', 'E'): 1,\n",
        " ('E', 'C'): 1,\n",
        " ('C', 'T'): 1,\n",
        " ('B', 'I'): 3,\n",
        " ('I', 'G'): 3,\n",
        " ('B', 'O'): 1,\n",
        " ('O', 'G'): 1,\n",
        " ('B', 'U'): 1,\n",
        " ('U', 'G'): 1,\n",
        " ('I', '</w>'): 2,\n",
        " ('l', 'o'): 1,\n",
        " ('o', 'v'): 1,\n",
        " ('v', 'e'): 1,\n",
        " ('e', '</w>'): 2,\n",
        " ('l', 'i'): 1,\n",
        " ('i', 'k'): 1,\n",
        " ('k', 'e'): 1,\n",
        " ('w', 'h'): 1,\n",
        " ('h', 'a'): 1,\n",
        " ('a', 't'): 1,\n",
        " ('t', '</w>'): 1,\n",
        " ('i', 's'): 1,\n",
        " ('s', '</w>'): 1}\n",
        " ```\n",
        "\n",
        " 위와 같이, 튜플로 묶인 두 개의 토큰 페어와, 각 페어별 등장횟수로 이루어진 딕셔너리가 반환되어야 합니다. 아래 코드를 실행해 결과값이 같은지 확인해보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W49OoJPX1NGv",
        "outputId": "8343186c-ff7f-4c17-b110-2c708c061270"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{('Y', 'B'): 8,\n",
              " ('B', 'A'): 2,\n",
              " ('A', 'G'): 2,\n",
              " ('G', 'T'): 7,\n",
              " ('T', 'A'): 8,\n",
              " ('A', '</w>'): 8,\n",
              " ('B', 'E'): 1,\n",
              " ('E', 'C'): 1,\n",
              " ('C', 'T'): 1,\n",
              " ('B', 'I'): 3,\n",
              " ('I', 'G'): 3,\n",
              " ('B', 'O'): 1,\n",
              " ('O', 'G'): 1,\n",
              " ('B', 'U'): 1,\n",
              " ('U', 'G'): 1,\n",
              " ('I', '</w>'): 2,\n",
              " ('l', 'o'): 1,\n",
              " ('o', 'v'): 1,\n",
              " ('v', 'e'): 1,\n",
              " ('e', '</w>'): 2,\n",
              " ('l', 'i'): 1,\n",
              " ('i', 'k'): 1,\n",
              " ('k', 'e'): 1,\n",
              " ('w', 'h'): 1,\n",
              " ('h', 'a'): 1,\n",
              " ('a', 't'): 1,\n",
              " ('t', '</w>'): 1,\n",
              " ('i', 's'): 1,\n",
              " ('s', '</w>'): 1}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 다음 코드를 실행한 결과값을 위의 결과와 비교해서 함수를 잘 작성했는지 확인해보세요!\n",
        "pairs = 페어등장횟수(vocab)\n",
        "pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Qg5KZzr78E"
      },
      "source": [
        "다음으로, `페어합치기` 함수를 구현해봅시다. `페어합치기` 함수는 `페어등장횟수` 함수로 계산된 페어 빈도수를 바탕으로, BPE 로직에 맞게 vocab을 업데이트합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NHRf7TB41Rj3"
      },
      "outputs": [],
      "source": [
        "def 페어합치기(pairs, vocab):\n",
        "\n",
        "  ##### TODO #####\n",
        "  # 함수 안을 채워주세요 #\n",
        "  # 이미 작성된 코드는 지우시면 안됩니다! #\n",
        "  best_pair = max(pairs, key=pairs.get)\n",
        "  # Convert pair to string\n",
        "  bigram = ' '.join(best_pair)  # 'Y B'\n",
        "  replacement = ''.join(best_pair)  # 'YB'\n",
        "  new_vocab = {}\n",
        "\n",
        "  # 기존 vocab을 순회\n",
        "  for word, freq in vocab.items():\n",
        "    # bigram이 단어에 존재하는 경우\n",
        "    if bigram in word:\n",
        "        # bigram을 replacement로 대체\n",
        "        new_word = word.replace(bigram, replacement)\n",
        "        new_vocab[new_word] = new_vocab.get(new_word, 0) + freq\n",
        "    else:\n",
        "        # bigram이 단어에 존재하지 않는 경우\n",
        "        new_vocab[word] = new_vocab.get(word, 0) + freq\n",
        "\n",
        "  return new_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "작성한 `페어합치기` 함수에 우리의 예시를 넣으면 다음과 같은 결과를 반환합니다.\n",
        "\n",
        "```\n",
        "{'YB A G T A </w>': 2,\n",
        " 'YB E C T A </w>': 1,\n",
        " 'YB I G T A </w>': 3,\n",
        " 'YB O G T A </w>': 1,\n",
        " 'YB U G T A </w>': 1,\n",
        " 'I </w>': 2,\n",
        " 'l o v e </w>': 1,\n",
        " 'l i k e </w>': 1,\n",
        " 'w h a t </w>': 1,\n",
        " 'i s </w>': 1}\n",
        " ```\n",
        "\n",
        " 아래 코드를 실행해 결과값이 같은지 확인해보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BETYArqds9P8",
        "outputId": "21365658-190a-4602-949e-b98e569ce4bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'YB A G T A </w>': 2,\n",
              " 'YB E C T A </w>': 1,\n",
              " 'YB I G T A </w>': 3,\n",
              " 'YB O G T A </w>': 1,\n",
              " 'YB U G T A </w>': 1,\n",
              " 'I </w>': 2,\n",
              " 'l o v e </w>': 1,\n",
              " 'l i k e </w>': 1,\n",
              " 'w h a t </w>': 1,\n",
              " 'i s </w>': 1}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 다음 코드를 실행한 결과값을 위의 결과와 비교해서 함수를 잘 작성했는지 확인해보세요!\n",
        "vocab = 페어합치기(pairs,vocab)\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_oFDna61v2A"
      },
      "source": [
        "## 2. BPE Tokenizer Train (30점)\n",
        "\n",
        "이제 BPE 트레이닝을 위한 각 파트 구현을 완료했습니다! **각각의 함수를 합쳐서 BPE Tokenizer의 학습을 수행하는 `학습` 함수를 작성해주세요. (15점)**\n",
        "\n",
        " - 1번 파트에서 작성한 함수들은 한번 vocab을 업데이트하는 데 필요한 함수들입니다.\n",
        " - `n_iter`인자(int type)를 받아, 입력받은 횟수만큼 vocab을 업데이트하도록 함수를 작성해주세요.\n",
        "\n",
        "1번 파트에서 모든 함수를 잘 작성했다면, 트레이닝 함수는 간단히 작성하실 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OIdnzNDDoP1j"
      },
      "outputs": [],
      "source": [
        "def 학습(corpus, n_iter):\n",
        "  ##### TODO #####\n",
        "  # 함수 안을 채워주세요 #\n",
        "  # 이미 작성된 코드는 지우시면 안됩니다! #\n",
        "  tokens = 사전만들기(corpus)\n",
        "\n",
        "  for _ in range(n_iter):\n",
        "    pairs = 페어등장횟수(vocab)\n",
        "    if not pairs:\n",
        "      break\n",
        "    vocab = 페어합치기(pairs, vocab)\n",
        "    \n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7-6x84hxhpM"
      },
      "source": [
        "학습 함수에 sample corpus를 넣고, `n_iter`을 10으로 정한 후 시행한 결과는 다음과 같습니다.\n",
        "\n",
        "```\n",
        "{'YBAGTA</w>': 2,\n",
        " 'YB': 3,\n",
        " 'E': 1,\n",
        " 'C': 1,\n",
        " 'TA</w>': 1,\n",
        " 'YBIGTA</w>': 3,\n",
        " 'O': 1,\n",
        " 'GTA</w>': 2,\n",
        " 'U': 1,\n",
        " 'I</w>': 2,\n",
        " 'l': 2,\n",
        " 'o': 1,\n",
        " 'v': 1,\n",
        " 'e</w>': 2,\n",
        " 'i': 2,\n",
        " 'k': 1,\n",
        " 'w': 1,\n",
        " 'h': 1,\n",
        " 'a': 1,\n",
        " 't': 1,\n",
        " '</w>': 2,\n",
        " 's': 1}\n",
        " ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk1iZqkJv1YU",
        "outputId": "725ca289-d10a-4388-bbc4-3208d8363438"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'YBAGTA</w>': 2,\n",
              " 'YB E C TA</w>': 1,\n",
              " 'YBIGTA</w>': 3,\n",
              " 'YB O GTA</w>': 1,\n",
              " 'YB U GTA</w>': 1,\n",
              " 'I</w>': 2,\n",
              " 'l o v e</w>': 1,\n",
              " 'l i k e</w>': 1,\n",
              " 'w h a t </w>': 1,\n",
              " 'i s </w>': 1}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = 학습(sample_corpus,n_iter=10)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hQsGGeet65Q"
      },
      "source": [
        "**또, 직접 학습을 시켜봅시다. (15점)**\n",
        "\n",
        "\n",
        "학습용 corpus는 각자 자유롭게 정해주세요. (ex. 노래 가사 등등...)\n",
        "\n",
        "- 단, 빈도수 기반의 BPE 토크나이저의 효과적인 학습을 위해서, 학습 corpus는 다음 조건을 만족하면 더 좋겠죠?\n",
        "\n",
        "  - 같은 단어가 여러개 들어가있고 적당히 긴 텍스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "O1cnQyJs1mVm"
      },
      "outputs": [],
      "source": [
        "##### TODO #####\n",
        "# 리스트 안에 train corpus를 자유롭게 채워주세요 #\n",
        "\n",
        "train_corpus = ['New jeans',\n",
        "              'super natural',\n",
        "              'stormy night',\n",
        "              'cloudy sky',\n",
        "              'in a moment you and i',\n",
        "              'one more chance',\n",
        "              'second chance',\n",
        "              'golden moon',\n",
        "              'diamond stars',\n",
        "              \"it's supernatural\",\n",
        "              'in a moment we unite'\n",
        "              ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "bnJ9ynBfx5Nl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'N e w </w>': 1,\n",
              " 'j e an s</w>': 1,\n",
              " 'super </w>': 1,\n",
              " 'n a t u r a l </w>': 1,\n",
              " 's t o r m y</w>': 1,\n",
              " 'n i g h t </w>': 1,\n",
              " 'c l o u d y</w>': 1,\n",
              " 's k y</w>': 1,\n",
              " 'i n</w>': 1,\n",
              " 'a </w>': 1,\n",
              " 'mo m e n t </w>': 1,\n",
              " 'y o u </w>': 1,\n",
              " 'an d </w>': 1,\n",
              " 'i </w>': 1,\n",
              " 'o n e</w>': 1,\n",
              " 'mo r e</w>': 1,\n",
              " 'c h an c e</w>': 1,\n",
              " 'g o l d e n</w>': 1,\n",
              " 'mo o n</w>': 1,\n",
              " 'd i a mo n d </w>': 1,\n",
              " 's t a r s</w>': 1,\n",
              " \"i t ' s</w>\": 1,\n",
              " 'super n a t u r a l </w>': 1}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##### TODO #####\n",
        "# 여러분의 train_corpus를 구현한 BPE 토크나이저 학습 함수에 넣고, 결과값을 프린트하세요\n",
        "tokens = 학습(train_corpus,n_iter=10)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-d6MqpC5i8o"
      },
      "source": [
        "## 보너스 질문 ( 추가점수 10점 )\n",
        "BPE 방식의 한계점은 무엇이고, 이러한 한계점이 왜 나타나는지 고민해보고 그 이유를 자유롭게 설명해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-axLI1jC6EcS"
      },
      "source": [
        "답: \n",
        "BPE의 주요 이점은 희귀 단어에 대한 처리 효율성과 언어 모델의 어휘 크기를 줄일 수 있다는 것입니다. 하지만 BPE 방식에는 몇 가지 한계점이 있으며, 이러한 한계점은 특정한 이유들로 인해 발생합니다.\n",
        "\n",
        "1. 맥락 정보의 손실\n",
        "BPE는 주어진 텍스트를 압축하는 방식으로 동작하기 때문에, 단어를 서브워드 단위로 분할하여 처리합니다. 이로 인해 맥락(context)을 표현하는 데 제한이 있을 수 있습니다. 예를 들어, 동일한 서브워드가 서로 다른 맥락에서 등장할 때, BPE는 그 차이를 구분하지 못하고 동일한 서브워드를 할당할 수 있습니다. 이는 맥락에 따라 단어의 의미가 달라지는 언어, 특히 한국어나 중국어와 같은 언어에서 문제가 될 수 있습니다.\n",
        "\n",
        "2. 어휘 생성의 제한\n",
        "BPE는 기존의 데이터에 기반해 서브워드를 생성합니다. 즉, 훈련 데이터에 등장하지 않은 새로운 단어 또는 철자 조합이 발생할 경우, 해당 단어를 제대로 처리하지 못하고 잘못된 서브워드 조합으로 해석될 수 있습니다. 이는 특히 신조어나 기술 용어, 브랜드 이름과 같은 단어에서 문제가 됩니다. 새로운 단어를 효율적으로 학습하지 못하면 모델의 성능에 부정적인 영향을 미칠 수 있습니다.\n",
        "\n",
        "3. 서브워드 조합의 불확실성\n",
        "BPE는 훈련 과정에서 빈도 기반으로 서브워드를 결정하기 때문에, 동일한 단어가 다양한 서브워드 조합으로 분절될 수 있습니다. 이는 단어 간의 일관성을 해치고, 모델이 해당 단어를 일관되게 이해하는 데 어려움을 초래할 수 있습니다. 예를 들어, \"unhappiness\"라는 단어가 \"un\", \"happiness\"로 분절될 수도 있고, \"un\", \"happi\", \"ness\"로 분절될 수도 있습니다. 이러한 불확실성은 모델의 해석 일관성을 저해할 수 있습니다.\n",
        "\n",
        "4. 문화적/언어적 특수성 반영의 어려움\n",
        "BPE는 특정 언어에 대한 세밀한 특성을 반영하기 어렵습니다. 예를 들어, 한국어의 경우 조사나 어미 변화를 통해 문법적 역할이 달라지는데, BPE는 이러한 변화를 적절히 반영하지 못할 수 있습니다. 또한, 한자나 복합어와 같이 의미가 여러 층위로 결합된 단어들을 처리하는 데 한계가 있을 수 있습니다. 이는 한국어나 일본어, 중국어와 같은 언어에서 특히 두드러지며, BPE가 서구 언어 중심의 접근 방식임을 시사합니다.\n",
        "\n",
        "5. 모델 훈련 시간 증가\n",
        "BPE는 훈련 데이터에 따라 서브워드 단위를 생성하기 때문에, 훈련 과정에서 추가적인 연산이 필요합니다. 이로 인해 초기 모델 훈련 시 시간이 더 소요될 수 있으며, 특히 대규모 데이터셋을 사용할 경우 이 문제가 더 두드러질 수 있습니다.\n",
        "\n",
        "결론:\n",
        "BPE 방식의 한계점은 주로 맥락 정보 손실, 새로운 단어 생성의 제한, 서브워드 조합의 불확실성, 언어적 특수성 반영의 어려움 등에서 기인합니다. 이러한 문제는 BPE가 텍스트를 분절하는 방식이 서브워드 단위로 이루어지기 때문에 발생하며, 언어의 복잡성 및 문화적 다양성을 충분히 반영하지 못하기 때문에 발생하는 것입니다. 따라서 BPE의 한계를 보완하기 위해 다른 기법들과 함께 사용되거나, 언어의 특수성을 더 잘 반영할 수 있는 새로운 방법론들이 연구되고 있습니다."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
